{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeS0ySGdbCj6"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/drive/MyDrive/Places2_simp.zip -d /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCgrADb-d2ed"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "image = Image.open('/content/drive/MyDrive/Places2_simp/airport_terminal/00000001.jpg')\n",
        "image.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSB0tbOKtfc_"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the target size for the resized images\n",
        "target_size = (256, 256)\n",
        "\n",
        "# Define the transforms for data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomRotation(degrees=20),\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(target_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Set the path to the dataset directory\n",
        "dataset_dir = '/content/drive/MyDrive/Places2_simp'\n",
        "\n",
        "# Get a list of all image file paths in the dataset directory\n",
        "file_paths = []\n",
        "class_names = sorted(os.listdir(dataset_dir))\n",
        "for class_name in class_names:\n",
        "    class_dir = os.path.join(dataset_dir, class_name)\n",
        "    for file_name in os.listdir(class_dir):\n",
        "        file_path = os.path.join(class_dir, file_name)\n",
        "        file_paths.append((file_path, class_names.index(class_name)))\n",
        "\n",
        "# Split the dataset into a training and validation set\n",
        "train_file_paths, val_file_paths = train_test_split(file_paths, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a function to load an image and apply the data augmentation transforms\n",
        "def load_image(file_path, transform):\n",
        "    # Open the image file\n",
        "    image = Image.open(file_path)\n",
        "\n",
        "    # Apply the data augmentation transforms\n",
        "    if transform:\n",
        "        image = transform(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Define a function to create a data loader object for a given set of file paths\n",
        "def create_data_loader(file_paths, transform, batch_size, shuffle):\n",
        "    return DataLoader(\n",
        "        [(file_path, label) for file_path, label in file_paths],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=2,\n",
        "        collate_fn=lambda batch: (\n",
        "            torch.stack([load_image(file_path, transform) for file_path, _ in batch]),\n",
        "            torch.tensor([label for _, label in batch])\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Create data loader objects for the training and validation sets\n",
        "train_loader = create_data_loader(train_file_paths, transform_train, batch_size=16, shuffle=True)\n",
        "val_loader = create_data_loader(val_file_paths, transform_test, batch_size=16, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnIDing2mtuN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "dir_path = '/content/drive/MyDrive/Places2_simp'\n",
        "if not os.path.exists(dir_path):\n",
        "    print(f\"The directory {dir_path} does not exist.\")\n",
        "else:\n",
        "    print(f\"The directory {dir_path} exists.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftVnLN7hyV2z"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of images in the training set: {len(train_file_paths)}\")\n",
        "print(f\"Number of images in the validation set: {len(val_file_paths)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuMLfPDr4Ykg"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.resnet import resnet34\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "# Load the ResNet-34 model and remove the last layers\n",
        "resnet34 = models.resnet34(pretrained=True)\n",
        "modules = list(resnet34.children())[:-2]\n",
        "backbone = nn.Sequential(*modules)\n",
        "\n",
        "# Define the new layers for classification\n",
        "num_classes = 40\n",
        "fc_layers = nn.Sequential(\n",
        "    nn.AdaptiveAvgPool2d((1, 1)),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Linear(256, num_classes)\n",
        ")\n",
        "\n",
        "# Combine the ResNet-34 backbone with the new layers\n",
        "model = nn.Sequential(\n",
        "    backbone,\n",
        "    fc_layers\n",
        ")\n",
        "\n",
        "\n",
        "# Set a learning rate\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': backbone.parameters(), 'lr': 0.0001},\n",
        "    {'params': fc_layers.parameters(), 'lr': 0.0001},\n",
        "])\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "# Check if a GPU is available and set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "resnet = models.resnet34(weights=models.resnet.ResNet34_Weights.IMAGENET1K_V1)\n",
        "\n",
        "for param in resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_classes = 40\n",
        "\n",
        "# Get the number of input features for the last layer\n",
        "num_features = resnet.fc.in_features\n",
        "\n",
        "# Replace the last layer with a new fully connected layer\n",
        "resnet.fc = nn.Linear(num_features, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW([\n",
        "    {'params': resnet.fc.parameters(), 'lr': 0.01},  # Learning rate for new layers\n",
        "    {'params': resnet.layer4.parameters(), 'lr': 0.001},  # Learning rate for transferred layers\n",
        "    {'params': resnet.layer3.parameters(), 'lr': 0.001},  # Learning rate for transferred layers\n",
        "    {'params': resnet.layer2.parameters(), 'lr': 0.001},  # Learning rate for transferred layers\n",
        "    {'params': resnet.layer1.parameters(), 'lr': 0.001},  # Learning rate for transferred layers\n",
        "    {'params': resnet.conv1.parameters(), 'lr': 0.001},  # Learning rate for transferred layers\n",
        "], lr=0.001, betas=(0.9, 0.999), eps=1e-8, amsgrad=True)\n",
        "\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Move the model to the device\n",
        "resnet = resnet.to(device)"
      ],
      "metadata": {
        "id": "oR1tmOAwd1GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JBSJRmz9Lsa"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH2khH1BCZdT"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Create a SummaryWriter object\n",
        "writer = SummaryWriter('logs')\n",
        "\n",
        "# Define the number of epochs and the batch size\n",
        "num_epochs = 10\n",
        "batch_size = 4\n",
        "\n",
        "# Loop over the data and train the model\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    print('-' * 40)\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    resnet.train() # set the model to training model\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        # Move the data and target tensors to the device\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = resnet(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Log the loss\n",
        "        writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        train_total += target.size(0)\n",
        "        train_correct += (predicted == target).sum().item()\n",
        "\n",
        "        # Calculate batch accuracy and print training statistics\n",
        "        batch_accuracy = 100 * (predicted == target).sum().item() / target.size(0)\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Train Batch {batch_idx}/{len(train_loader)} - loss: {loss.item():.4f} - batch accuracy: {batch_accuracy:.2f}%')\n",
        "\n",
        "    # Log the average training loss and accuracy\n",
        "    train_loss /= len(train_loader)\n",
        "    train_accuracy = 100 * train_correct / train_total\n",
        "    writer.add_scalar('Loss/train_avg', train_loss, epoch)\n",
        "    writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
        "    print(f'Train loss: {train_loss:.4f} - Train accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    resnet.eval() # set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            # Move the data and target tensors to the device\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = resnet(data)\n",
        "            loss = criterion(output, target)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += (predicted == target).sum().item()\n",
        "\n",
        "    # Log the average validation loss and accuracy\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = 100 * val_correct / val_total\n",
        "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
        "    writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
        "    print(f'Validation loss: {val_loss:.4f} - Validation accuracy: {val_accuracy:.2f}%')\n",
        "    print('=' * 40)\n",
        "\n",
        "    # Update the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "# Close the SummaryWriter\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "# Create a SummaryWriter object\n",
        "writer = SummaryWriter('logs')\n",
        "\n",
        "# Define the number of epochs and the batch size\n",
        "num_epochs = 6\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "# Loop over the data and train the model\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    print('-' * 40)\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    model.train() # set the model to training mode\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        try:\n",
        "            # Move the data and target tensors to the device\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Log the loss\n",
        "            writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            train_total += target.size(0)\n",
        "            train_correct += (predicted == target).sum().item()\n",
        "\n",
        "            # Calculate batch accuracy and print training statistics\n",
        "            batch_accuracy = 100 * (predicted == target).sum().item() / target.size(0)\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f'Train Batch {batch_idx}/{len(train_loader)} - loss: {loss.item():.4f} - batch accuracy: {batch_accuracy:.2f}%')\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f'File not found. Skipping batch {batch_idx}/{len(train_loader)}.')\n",
        "\n",
        "    # Log the average training loss and accuracy\n",
        "    train_loss /= len(train_loader)\n",
        "    train_accuracy = 100 * train_correct / train_total\n",
        "    writer.add_scalar('Loss/train_avg', train_loss, epoch)\n",
        "    writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
        "    print(f'Train loss: {train_loss:.4f} - Train accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    model.eval() # set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            # Move the data and target tensors to the device\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += (predicted == target).sum().item()\n",
        "\n",
        "    # Log the average validation loss and accuracy\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = 100 * val_correct / val_total\n",
        "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
        "    writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
        "    print(f'Validation loss: {val_loss:.4f} - Validation accuracy: {val_accuracy:.2f}%')\n",
        "    print('=' * 40)\n",
        "\n",
        "    # Update the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "# Close the SummaryWriter\n",
        "writer.close()\n"
      ],
      "metadata": {
        "id": "OjNfJTpsma7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "top1_correct = 0\n",
        "top5_correct = 0\n",
        "total = 0\n",
        "predicted_labels = []\n",
        "target_labels = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data, target in val_loader:\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        _, top5_predicted = torch.topk(output, 5, dim=1)\n",
        "\n",
        "        total += target.size(0)\n",
        "        top1_correct += (predicted == target).sum().item()\n",
        "        top5_correct += sum([(target[i] in top5_predicted[i]) for i in range(len(target))])\n",
        "\n",
        "        predicted_labels.extend(predicted.tolist())\n",
        "        target_labels.extend(target.tolist())\n",
        "top1_accuracy = 100 * top1_correct / total\n",
        "top5_accuracy = 100 * top5_correct / total\n",
        "confusion = confusion_matrix(target_labels, predicted_labels)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class_names = [\"airport_terminal\" , \"amphithetre\" , \"amusement_park\", \"art_gallery\", \"bakery_shop\", \"bar\", \"bookstore\", \"botanical_garden\",\n",
        "               \"bridge\", \"bus_interior\", \"butshers_shop\", \"campsite\", \"classroom\", \"coffee_shop\", \"construction_site\", \"courtyard\", \"driveway\",\n",
        "               \"fire_station\", \"fountain\", \"gas_station\", \"harbour\", \"highway\", \"kindergarden_classroom\", \"lobby\", \"market_outdoor\", \"museum\",\n",
        "               \"office\", \"parking_lot\", \"ailroad_track\", \"restaurant\", \"river\", \"shed\", \"staircase\", \"supermarket\", \"swimming_pool_outdoor\",\n",
        "               \"track\", \"valley\",\"yard\"]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "num_samples = 5  # Number of samples to display\n",
        "random_indices = np.random.choice(range(len(target_labels)), size=num_samples, replace=False)\n",
        "\n",
        "for index in random_indices:\n",
        "    image = val_dataset[index][0]  # Get the image tensor\n",
        "    actual_class = class_names[target_labels[index]]  # Get the actual class name\n",
        "    predicted_classes = [class_names[pred] for pred in top5_predicted[index]]  # Get the predicted class names\n",
        "\n",
        "    # Display the image and class names\n",
        "    plt.imshow(image.permute(1, 2, 0))\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Top-5 Predicted: {predicted_classes}\\nActual: {actual_class}')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "mdppZpjUnNgN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}